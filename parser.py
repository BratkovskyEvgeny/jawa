import re
import time
from typing import Dict, List, Optional
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup

import config


class MotorcycleParser:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update(
            {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
            }
        )

    def parse_site(self, site_key: str) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å–∞–π—Ç–∞"""
        site_config = config.PARSING_SITES.get(site_key)
        if not site_config:
            print(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è —Å–∞–π—Ç–∞ {site_key} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return []

        try:
            print(f"–ü–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–∞: {site_config['name']}")

            # –ü–æ–ª—É—á–∞–µ–º HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            response = self.session.get(site_config["search_url"], timeout=30)
            response.raise_for_status()

            # –ü–∞—Ä—Å–∏–º HTML
            soup = BeautifulSoup(response.content, "html.parser")

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ–±—ä—è–≤–ª–µ–Ω–∏—è
            ads = self._extract_ads(soup, site_config)

            # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º Jawa –∏ CZ
            filtered_ads = self._filter_jawa_cz_ads(ads)

            print(
                f"–ù–∞–π–¥–µ–Ω–æ {len(filtered_ads)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π Jawa/CZ –Ω–∞ {site_config['name']}"
            )

            # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–∞–π—Ç–µ
            for ad in filtered_ads:
                ad["site_name"] = site_config["name"]
                ad["site_key"] = site_key

            return filtered_ads

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {site_config['name']}: {e}")
            return []

    def _extract_ads(self, soup: BeautifulSoup, site_config: Dict) -> List[Dict]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–π —Å HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        ads = []
        selectors = site_config["selectors"]

        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã –æ–±—ä—è–≤–ª–µ–Ω–∏–π
        ad_elements = soup.select(selectors["items"])

        for element in ad_elements[: config.MAX_ITEMS_PER_SITE]:
            try:
                ad = {}

                # –ó–∞–≥–æ–ª–æ–≤–æ–∫
                title_elem = element.select_one(selectors["title"])
                if title_elem:
                    ad["title"] = title_elem.get_text(strip=True)

                # –¶–µ–Ω–∞
                price_elem = element.select_one(selectors["price"])
                if price_elem:
                    ad["price"] = price_elem.get_text(strip=True)

                # –°—Å—ã–ª–∫–∞
                link_elem = element.select_one(selectors["link"])
                if link_elem:
                    link = link_elem.get("href", "")
                    if link:
                        ad["link"] = urljoin(site_config["base_url"], link)

                # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                img_elem = element.select_one(selectors["image"])
                if img_elem:
                    img_src = img_elem.get("src") or img_elem.get("data-src")
                    if img_src:
                        ad["image_url"] = urljoin(site_config["base_url"], img_src)

                # –û–ø–∏—Å–∞–Ω–∏–µ (–µ—Å–ª–∏ –µ—Å—Ç—å)
                desc_elem = element.select_one("p, .description, .desc")
                if desc_elem:
                    ad["description"] = desc_elem.get_text(strip=True)

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —É –Ω–∞—Å –µ—Å—Ç—å –º–∏–Ω–∏–º—É–º –¥–∞–Ω–Ω—ã—Ö
                if ad.get("title") and ad.get("link"):
                    ads.append(ad)

            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏—è: {e}")
                continue

        return ads

    def _filter_jawa_cz_ads(self, ads: List[Dict]) -> List[Dict]:
        """–§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –æ–±—ä—è–≤–ª–µ–Ω–∏–π –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º Jawa –∏ CZ"""
        filtered_ads = []

        for ad in ads:
            title = ad.get("title", "").lower()
            description = ad.get("description", "").lower()

            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞
            jawa_keywords = [
                "jawa",
                "—è–≤–∞",
                "cezet",
                "—á–µ–∑–µ—Ç",
                "cz",
                "—á–µ—Ö–æ—Å–ª–æ–≤–∞–∫–∏—è",
                "—á–µ—à—Å–∫–∏–π",
            ]

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            for keyword in jawa_keywords:
                if keyword in title or keyword in description:
                    filtered_ads.append(ad)
                    break

            # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ –æ—Å–Ω–æ–≤–Ω—ã–º –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞
            for keyword in config.SEARCH_KEYWORDS:
                if keyword.lower() in title or keyword.lower() in description:
                    if ad not in filtered_ads:  # –ò–∑–±–µ–≥–∞–µ–º –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
                        filtered_ads.append(ad)
                    break

        return filtered_ads

    def parse_all_sites(self) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã—Ö —Å–∞–π—Ç–æ–≤"""
        all_ads = []

        for site_key in config.PARSING_SITES.keys():
            try:
                print(f"–ü–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–∞: {config.PARSING_SITES[site_key]['name']}")
                site_ads = self.parse_site(site_key)
                all_ads.extend(site_ads)
                print(
                    f"–ù–∞–π–¥–µ–Ω–æ {len(site_ads)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π Jawa/CZ –Ω–∞ {config.PARSING_SITES[site_key]['name']}"
                )

                # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –∫ —Ä–∞–∑–Ω—ã–º —Å–∞–π—Ç–∞–º
                if (
                    site_key != list(config.PARSING_SITES.keys())[-1]
                ):  # –ù–µ –∂–¥–µ–º –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–∞–π—Ç–∞
                    time.sleep(config.REQUEST_DELAY)

            except Exception as e:
                print(
                    f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {config.PARSING_SITES[site_key]['name']}: {e}"
                )
                continue

        return all_ads

    def search_specific_model(self, model: str) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –º–æ—Ç–æ—Ü–∏–∫–ª–æ–≤ Jawa –∏ CZ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        print(f"üîç –ù–∞—á–∏–Ω–∞—é –ø–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É: '{model}'")

        all_ads = self.parse_all_sites()
        print(f"üìä –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–π: {len(all_ads)}")

        if all_ads:
            print(f"üìù –ü—Ä–∏–º–µ—Ä –ø–µ—Ä–≤–æ–≥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏—è: {all_ads[0]}")

        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º Jawa –∏ CZ
        model_ads = []
        model_lower = model.lower()

        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ (—Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –Ω–∞–ø–∏—Å–∞–Ω–∏—è)
        jawa_keywords = [
            "jawa",
            "—è–≤–∞",
            "cezet",
            "—á–µ–∑–µ—Ç",
            "cz",
            "JAWA",
            "–Ø–í–ê",
            "CEZET",
            "–ß–ï–ó–ï–¢",
            "CZ",
            "Jawa",
            "–Ø–≤–∞",
            "Cezet",
            "–ß–µ–∑–µ—Ç",
        ]

        print(f"üîë –ò—â—É –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º: {jawa_keywords[:5]}...")

        for i, ad in enumerate(all_ads):
            title = ad.get("title", "").lower()
            description = ad.get("description", "").lower()

            print(f"üìã –û–±—ä—è–≤–ª–µ–Ω–∏–µ {i+1}: title='{title[:50]}...', description='{description[:50]}...'")

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º Jawa/CZ (–∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏)
            for keyword in jawa_keywords:
                if keyword.lower() in title or keyword.lower() in description:
                    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ '{keyword}' –≤ –æ–±—ä—è–≤–ª–µ–Ω–∏–∏ {i+1}")
                    model_ads.append(ad)
                    break

        print(f"üéØ –ò—Ç–æ–≥–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: –Ω–∞–π–¥–µ–Ω–æ {len(model_ads)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
        return model_ads


class AdvancedParser(MotorcycleParser):
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä —Å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏"""

    def __init__(self):
        super().__init__()
        self.session.headers.update(
            {
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                "Accept-Language": "ru-RU,ru;q=0.8,en-US;q=0.5,en;q=0.3",
                "Accept-Encoding": "gzip, deflate",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1",
            }
        )

    def parse_with_retry(self, site_key: str, max_retries: int = 3) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ —Å –ø–æ–≤—Ç–æ—Ä–Ω—ã–º–∏ –ø–æ–ø—ã—Ç–∫–∞–º–∏ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö"""
        for attempt in range(max_retries):
            try:
                return self.parse_site(site_key)
            except Exception as e:
                print(f"–ü–æ–ø—ã—Ç–∫–∞ {attempt + 1} –¥–ª—è {site_key} –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
                if attempt < max_retries - 1:
                    time.sleep(config.REQUEST_DELAY * (attempt + 1))
                else:
                    print(f"–í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –¥–ª—è {site_key} –∏—Å—á–µ—Ä–ø–∞–Ω—ã")
                    return []

    def get_ad_details(self, ad_url: str) -> Optional[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –æ–±—ä—è–≤–ª–µ–Ω–∏–∏"""
        try:
            response = self.session.get(ad_url, timeout=30)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, "html.parser")

            details = {}

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–µ—Ç–∞–ª–∏
            # –ì–æ–¥ –≤—ã–ø—É—Å–∫–∞
            year_pattern = r"\b(19[5-9]\d|20[0-2]\d)\b"
            text_content = soup.get_text()
            year_match = re.search(year_pattern, text_content)
            if year_match:
                details["year"] = year_match.group(1)

            # –ü—Ä–æ–±–µ–≥
            mileage_pattern = r"(\d+)\s*(–∫–º|—Ç—ã—Å\.?\s*–∫–º|—Ç—ã—Å—è—á\s*–∫–º)"
            mileage_match = re.search(mileage_pattern, text_content, re.IGNORECASE)
            if mileage_match:
                details["mileage"] = (
                    mileage_match.group(1) + " " + mileage_match.group(2)
                )

            # –°–æ—Å—Ç–æ—è–Ω–∏–µ
            condition_keywords = [
                "–æ—Ç–ª–∏—á–Ω–æ–µ",
                "—Ö–æ—Ä–æ—à–µ–µ",
                "—É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω–æ–µ",
                "—Ç—Ä–µ–±—É–µ—Ç —Ä–µ–º–æ–Ω—Ç–∞",
            ]
            for keyword in condition_keywords:
                if keyword in text_content.lower():
                    details["condition"] = keyword
                    break

            return details

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–µ—Ç–∞–ª–µ–π –æ–±—ä—è–≤–ª–µ–Ω–∏—è {ad_url}: {e}")
            return None
